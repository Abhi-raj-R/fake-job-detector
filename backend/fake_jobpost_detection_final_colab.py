# -*- coding: utf-8 -*-
"""Fake_jobpost_detection_Final_Colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UnpkUL48IrPNpWJayu7ew8RFHQbBdwhD
"""

!pip install wordcloud

!pip install -U spacy

"""**SETUP AND IMPORTS**"""

import string
import re
import numpy as np
import pandas as pd
import random
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.base import TransformerMixin
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
from wordcloud import WordCloud
import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from spacy.lang.en import English

"""**DATA LOADING**"""

df=pd.read_csv("/content/fake_job_postings.csv")
df.head()

df.shape

df.isnull().sum()

"""**DATA CLEANING AND PREPROCESSING**"""

columns=["job_id","telecommuting","has_company_logo","has_questions","salary_range","employment_type"]
 for colu in columns:
  del df[colu]

df.head()

df.fillna("",inplace=True)

"""**DATA VISUALIZATION**"""

plt.figure(figsize=(15,5))
sns.countplot(y="fraudulent",data=df)
plt.show()

df.groupby("fraudulent")["fraudulent"].count()

exp = dict(df.required_experience.value_counts())
del exp[""]
exp

plt.figure(figsize=(10,5))
sns.set_theme(style="whitegrid")
plt.bar(exp.keys(), exp.values())
plt.title("No. of Jobs with Experience",size=20)
plt.xlabel("Experience",size=10)
plt.ylabel("No. of Jobs",size=10)
plt.xticks(rotation=30)
plt.show()

def split(location):
  l=location.split(",")
  return l[0]
df["country"]=df["location"].apply(split)

df.head()

countr= dict(df.country.value_counts()[:14])
del countr[""]
countr

plt.figure(figsize=(8,6))
plt.bar(list(countr.keys()),countr.values())
plt.title("Country-wise Job Posting",size=20)
plt.xlabel("Countries",size=10)
plt.ylabel("No. of Jobs",size=10)
plt.show()

edu=dict(df.required_education.value_counts()[:7])
del edu[""]
edu

plt.figure(figsize=(15,6))
plt.title("Job Postings based on Education",size=20)
plt.bar(edu.keys(),edu.values())
plt.xlabel("Education",size=10)
plt.ylabel("No. of Jobs",size=10)

print(df[df.fraudulent==0].title.value_counts()[:10])

print(df[df.fraudulent==1].title.value_counts()[:10])

df["text"]=df["title"]+" "+df["company_profile"]+" "+df["description"]+" "+df["requirements"]+" "+df["benefits"]
del df['title']
del df['location']
del df['department']
del df['company_profile']
del df['description']
del df['requirements']
del df['benefits']
del df['required_experience']
del df['required_education']
del df['industry']
del df['function']
del df['country']

df.head()

fraudjobs_text=df[df.fraudulent==1].text
realjobs_text=df[df.fraudulent==0].text

"""FAKE JOB WORDCLOUD"""

STOPWORDS=spacy.lang.en.stop_words.STOP_WORDS
plt.figure(figsize=(16,14))
wc=WordCloud(min_font_size = 3, max_words = 3000, width = 1600, height = 800, stopwords = STOPWORDS).generate(str(" ".join(fraudjobs_text)))
plt.imshow(wc,interpolation="bilinear")

"""REAL JOB WORDCLOUD"""

STOPWORDS=spacy.lang.en.stop_words.STOP_WORDS
plt.figure(figsize=(16,14))
wc=WordCloud(min_font_size = 3, max_words = 3000, width = 1600, height = 800, stopwords = STOPWORDS).generate(str(" ".join(realjobs_text)))
plt.imshow(wc,interpolation="bilinear")

!pip install spacy && python -m spacy download en

"""**TEXT CLEANING AND TOKENIZATION**"""

punctuation=string.punctuation

nlp=spacy.load("en_core_web_sm")
stop_words=spacy.lang.en.stop_words.STOP_WORDS

parser=English()

def spacy_tokenizer(sentence):
  mytokens=parser(sentence)

  mytokens = [ word.lemma_.lower().strip() if word.lemma_ != "-PRON-" else word.lower_
                for word in mytokens ]

  mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuation ]

  return mytokens


class predictors(TransformerMixin):
    def transform(self, X, **transform_params):
        return [clean_text(text) for text in X]

    def fit(self, X, y=None, **fit_params):
        return self

    def get_params(self, deep=True):
        return {}

def clean_text(text):
  return text.strip().lower()

df["text"]=df["text"].apply(clean_text)

"""**NUMERICAL FEATURE EXTRACTION**"""

cv=TfidfVectorizer(max_features=3000,ngram_range=(1, 2),
    stop_words='english')
x=cv.fit_transform(df["text"])
df1=pd.DataFrame(x.toarray(), columns=cv.get_feature_names_out())
df.drop(["text"],axis=1,inplace=True)
main_df=pd.concat([df1,df],axis=1)
Y = main_df["fraudulent"]
X = main_df.drop("fraudulent", axis=1)

# 5. Split into train/test
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

main_df.head()

Y = main_df.iloc[:, -1]
X = main_df.iloc[:, :-1]

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

"""**DATA SPLITTING AND BALANCING**"""

from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

"""PCA VISUALIZATION: REAL VS FAKE JOBS"""

from sklearn.decomposition import PCA
import seaborn as sns
import matplotlib.pyplot as plt

# Use NumPy array from the DataFrame
X_array = X.values  # or: X_train.values if you want training set only

# Run PCA
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X_array)

# Plot with seaborn
plt.figure(figsize=(10,7))
sns.scatterplot(x=X_reduced[:, 0], y=X_reduced[:, 1], hue=Y, palette=["green", "red"])
plt.title("PCA Visualization of Job Descriptions")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.legend(title="Class (0=Real, 1=Fake)")
plt.show()

"""**MODEL TRAINING AND EVALUATION**

LOGISTIC REGRESSION
"""

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(class_weight='balanced', max_iter=1000)
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

print("Logistic Regression:\n", classification_report(y_test, y_pred_lr))
print(confusion_matrix(y_test, y_pred_lr))
from sklearn.metrics import accuracy_score
print("Accuracy:", accuracy_score(y_test, y_pred_lr))

"""TF-IDF: MOST IMPRTANT WORDS FOR PREDICTING FAKE AND REAL JOBS"""

import numpy as np
import matplotlib.pyplot as plt

feature_names = cv.get_feature_names_out()
coefficients = lr.coef_[0]

# Top 20 real job words
top_real_idx = np.argsort(coefficients)[:20]
top_real_words = feature_names[top_real_idx]
top_real_weights = coefficients[top_real_idx]

# Top 20 fake job words
top_fake_idx = np.argsort(coefficients)[-20:]
top_fake_words = feature_names[top_fake_idx]
top_fake_weights = coefficients[top_fake_idx]

# Plot Real Job Words
plt.figure(figsize=(10,5))
plt.barh(top_real_words, top_real_weights, color='green')
plt.title("Top Words Associated with Real Jobs")
plt.xlabel("Weight (TF-IDF Coefficients)")
plt.show()

# Plot Fake Job Words
plt.figure(figsize=(10,5))
plt.barh(top_fake_words, top_fake_weights, color='red')
plt.title("Top Words Associated with Fake Jobs")
plt.xlabel("Weight (TF-IDF Coefficients)")
plt.show()

"""RANDOM FOREST CLASSIFIER"""

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(
     n_estimators=200,            # More trees = more robust ensemble
    criterion="entropy",          # Still fine for splitting based on information gain
    class_weight="balanced",      # Compensates for imbalanced dataset
    oob_score=True,               # Enables out-of-bag error estimation
    n_jobs=-1,                    # Uses all available CPU cores (faster)
    random_state=42               # For reproducibility
)

model = rfc.fit(X_train_smote, y_train_smote)

y_pred = rfc.predict(X_test)

print(X_test)

pred=rfc.predict(X_test)
score=accuracy_score(y_test,pred)
score

"""XGBOOST CLASSIFIER


"""

# Install XGBoost if not already
!pip install xgboost

from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Train XGBoost
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb.fit(X_train_smote, y_train_smote)

# Predict
y_pred_xgb = xgb.predict(X_test)

# Evaluate
print("XGBoost Results:\n")
print("Accuracy:", accuracy_score(y_test, y_pred_xgb))
print("Classification Report:\n", classification_report(y_test, y_pred_xgb))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_xgb))

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

y_prob = xgb.predict_proba(X_test)[:, 1]
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = roc_auc_score(y_test, y_prob)

plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, label=f'XGBoost (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - XGBoost")
plt.legend()
plt.show()

"""MULTINOMIAL NAIVE BAYES"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Train the model
nb = MultinomialNB()
nb.fit(X_train_smote, y_train_smote)

# Predict
y_pred_nb = nb.predict(X_test)

# Evaluate
print("Multinomial Naive Bayes Results:\n")
print("Accuracy:", accuracy_score(y_test, y_pred_nb))
print("Classification Report:\n", classification_report(y_test, y_pred_nb))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_nb))

"""LINEAR SVM"""

from sklearn.svm import LinearSVC

# Train the model
svm = LinearSVC()
svm.fit(X_train_smote, y_train_smote)

# Predict
y_pred_svm = svm.predict(X_test)

# Evaluate
print("Linear SVM Results:\n")
print("Accuracy:", accuracy_score(y_test, y_pred_svm))
print("Classification Report:\n", classification_report(y_test, y_pred_svm))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_svm))

from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

ConfusionMatrixDisplay.from_estimator(rfc, X_test, y_test, cmap="Blues")

"""**MODEL COMPARISON SUMMARY**

"""

y_pred_lr    # Logistic Regression
y_pred       # Random Forest
y_pred_xgb   # XGBoost
y_pred_nb    # Naive Bayes
y_pred_svm   # Linear SVM

#This table compares the accuracy, precision, recall, and F1-score of all five classifiers.

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import pandas as pd

# Build metrics table
model_metrics = [
    {
        "Model": "Logistic Regression",
        "Accuracy": round(accuracy_score(y_test, y_pred_lr), 4),
        "Precision": round(precision_score(y_test, y_pred_lr), 4),
        "Recall": round(recall_score(y_test, y_pred_lr), 4),
        "F1-Score": round(f1_score(y_test, y_pred_lr), 4)
    },
    {
        "Model": "Random Forest",
        "Accuracy": round(accuracy_score(y_test, y_pred), 4),
        "Precision": round(precision_score(y_test, y_pred), 4),
        "Recall": round(recall_score(y_test, y_pred), 4),
        "F1-Score": round(f1_score(y_test, y_pred), 4)
    },
    {
        "Model": "XGBoost",
        "Accuracy": round(accuracy_score(y_test, y_pred_xgb), 4),
        "Precision": round(precision_score(y_test, y_pred_xgb), 4),
        "Recall": round(recall_score(y_test, y_pred_xgb), 4),
        "F1-Score": round(f1_score(y_test, y_pred_xgb), 4)
    },
    {
        "Model": "Naive Bayes",
        "Accuracy": round(accuracy_score(y_test, y_pred_nb), 4),
        "Precision": round(precision_score(y_test, y_pred_nb), 4),
        "Recall": round(recall_score(y_test, y_pred_nb), 4),
        "F1-Score": round(f1_score(y_test, y_pred_nb), 4)
    },
    {
        "Model": "Linear SVM",
        "Accuracy": round(accuracy_score(y_test, y_pred_svm), 4),
        "Precision": round(precision_score(y_test, y_pred_svm), 4),
        "Recall": round(recall_score(y_test, y_pred_svm), 4),
        "F1-Score": round(f1_score(y_test, y_pred_svm), 4)
    }
]

# Convert to DataFrame for display
metrics_df = pd.DataFrame(model_metrics)
metrics_df.sort_values(by="F1-Score", ascending=False).reset_index(drop=True)

"""**MODEL SAVING**"""

import joblib

# Save trained model and vectorizer
joblib.dump(model, 'job_post_model.pkl')
joblib.dump(cv, 'vectorizer.pkl')

"""SAVING XG BOOST MODEL"""

import joblib

# Save model and TF-IDF vectorizer
joblib.dump(xgb, "xgb_fraud_detector.pkl")
joblib.dump(cv, "tfidf_vectorizer.pkl")

"""**LOAD THE SAVED MODEL AND VECTORIZER**"""

import joblib
import pandas as pd

# Load the trained model and vectorizer
loaded_model = joblib.load('job_post_model.pkl')
loaded_vectorizer = joblib.load('vectorizer.pkl')

"""**FUNCTION TO PREPROCESS NEW TEXT DATA USING THE LOADED VECTORIZER**"""

# Assuming you have the clean_text function and spacy_tokenizer defined from previous cells
# If not, you'll need to include those definitions here or ensure they are in your notebook's environment

def preprocess_new_text(text):
    cleaned_text = clean_text(text)
    # You might want to apply the same spacy_tokenizer if you used it before TF-IDF
    # For this example, we'll directly use the loaded_vectorizer which expects cleaned text
    # If your original TF-IDF was applied after spacy_tokenizer, you'd need to adjust this.
    return cleaned_text

"""**FUNCTION TO PREDICT ON NEW JOB POSTING TEXT**"""

def predict_fraudulent(job_post_text):
    # Preprocess the input text
    preprocessed_text = preprocess_new_text(job_post_text)

    # Transform the preprocessed text using the loaded vectorizer
    # The vectorizer expects an iterable (like a list) of strings
    text_vectorized = loaded_vectorizer.transform([preprocessed_text])

    # Make a prediction using the loaded model
    prediction = loaded_model.predict(text_vectorized)

    # Return the prediction (0 for non-fraudulent, 1 for fraudulent)
    return prediction[0]

"""**PREDICTION OF A NEW JOB DESCRIPTION**

"""

import joblib

# Load model and vectorizer
xgb_model = joblib.load("job_post_model.pkl")
tfidf = joblib.load("vectorizer.pkl")

# Sample prediction function
def predict_job_fraud(text):
    # Clean text
    text = text.strip().lower()

    # Transform
    vector = tfidf.transform([text])

    # Predict
    pred = xgb_model.predict(vector)[0]

    # Label
    label = "FAKE JOB" if pred == 1 else "REAL JOB"
    return label

# üîç Test it!
sample_text = ""
print("Prediction:", predict_job_fraud(sample_text))

# Example new job posting text
new_job_post = """
Work From Home Opportunity! üíº

We are hiring *immediately* for a remote data entry position. No experience needed!

‚úî Make up to $700 per day from the comfort of your home
‚úî Flexible hours ‚Äì work anytime, anywhere
‚úî All you need is an internet connection and a willingness to earn

üëâ To apply, simply email us your **full name**, **mobile number**, and a scanned **government ID**.
No background checks required. Limited slots available ‚Äì ACT FAST!

Join now and start earning instantly! üí∏

"""

prediction = predict_fraudulent(new_job_post)

if prediction == 1:
    print("This job posting is predicted to be FRAUDULENT.")
else:
    print("This job posting is predicted to be NON-FRAUDULENT.")